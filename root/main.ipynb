{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lattice Physics \n",
    "The model encompasses 2 lattice physics parameters\n",
    "1. **k -inf** : The infinite multiplication factor  \n",
    "2. **PPPF** : Pin Power Peaking Factor\n",
    "\n",
    "\n",
    "Both of these are modeled as functions of variations in fuel pin enrichments for the reactor. The model focuses on predicting the value of k-inf and PPPF values associated with enrichments. \n",
    "\n",
    "For further information on the reactor please refer - https://github.com/your-username/your-repo-name/blob/main/README.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing necessary libraries \n",
    "In the model the following library are used - \n",
    "1. Numpy\n",
    "2. Scikit-learn\n",
    "3. regression (custom linear regression library)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "from regression import regression_func\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.linear_model import Ridge\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data set contains 24000 rows and 1 Column, each column has multiple features seperated by whitespace. Pandas library is used to make data more readable into 24K rows and 41 columns.\n",
    "Since, it is not mentioned which columns correspond to which feature or which target variable the correct appraoch would be to actually find out mean, mode and median for all the columns because the normal values of k-inf lies in the range (0.8,1.5) while on the other hand the PPPF factor lies mainly in the range (1.5,2.2). The two columns which will satisfy these conditions will correspond to k-inf and PPPF target variable columns and rest will be the input features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Mean   Median  Standard Deviation    Mode      Max\n",
      "1.326300000  1.325139  1.32608            0.017798  1.3222  1.38558\n",
      "                Mean    Median  Standard Deviation      Mode       Max\n",
      "1.862085698  1.88356  1.876338             0.12002  1.527497  2.473015\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data/raw.csv', sep=r'\\s+')\n",
    "df_test = pd.read_csv('data/test.csv', sep=r'\\s+')\n",
    "\n",
    "pd.set_option('display.max_columns', None) \n",
    "pd.set_option('display.width', None) \n",
    "pd.set_option('display.max_rows', None)\n",
    "stats = pd.DataFrame({\n",
    "    'Mean': df.mean(),\n",
    "    'Median': df.median(),\n",
    "    'Standard Deviation': df.std(),\n",
    "    'Mode': df.mode().iloc[0],\n",
    "    'Max' : df.max()\n",
    "})\n",
    "X = (0.9, 1.5)\n",
    "Y = (1.5, 2.2)\n",
    "filtered_columns_1 = stats[\n",
    "    (stats['Mean'] >= X[0]) & \n",
    "    (stats['Median'] >= X[0]) & \n",
    "    (stats['Mode'] >= X[0]) &\n",
    "    (stats['Mean'] <= X[1]) & \n",
    "    (stats['Median'] <= X[1]) & \n",
    "    (stats['Mode'] <= X[1])\n",
    "]\n",
    "filtered_columns_2 = stats[\n",
    "    (stats['Mean'] >= Y[0]) & \n",
    "    (stats['Median'] >= Y[0]) & \n",
    "    (stats['Mode'] >= Y[0]) &\n",
    "    (stats['Mean'] <= Y[1]) & \n",
    "    (stats['Median'] <= Y[1]) & \n",
    "    (stats['Mode'] <= Y[1])\n",
    "]\n",
    "print(filtered_columns_1)\n",
    "print(filtered_columns_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalisation and Flattening\n",
    "The dataset is now split into 3 seperate parts belonging to - \n",
    "1. **k-inf**\n",
    "2. **PPPF**\n",
    "3. **Input Features**\n",
    "\n",
    "The unnormalised data varies between a greater range which might hamper effective learning of the model since convergence is reached in a much more stable fashion when data is normalised. \n",
    "The Normalisation technique used for this model is known as Z-Score Normalisation.\n",
    "\n",
    "The normalisation technique works using the following: $$ X_{\\text norm} = \\frac{X - \\mu}{\\sigma} $$\n",
    "\n",
    "where $$ \\mu = \\frac{\\sum_{i=1}^n x_i}{n}, \\quad \\sigma = \\sqrt{\\frac{\\sum_{i=1}^n (x_i - \\mu)^2}{n}} $$\n",
    "\n",
    "Since the data in numpy flows quite differently therefore all the (24000,1) column matrices are converted to arrays by reducing their dimensions from 2 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalisation(x):\n",
    "    mu = np.mean(x,axis=0)\n",
    "    sigma = np.std(x,axis=0)\n",
    "    x_norm = (x-mu)/sigma\n",
    "    return (x_norm)\n",
    "\n",
    "y_train_k_inf = (df.iloc[:, :1]).to_numpy()\n",
    "y_train_pppf = (df.iloc[:, 1:2]).to_numpy()\n",
    "x_train = (df.iloc[:, 2:]).to_numpy()\n",
    "\n",
    "x_train = (normalisation(x_train))\n",
    "y_train_k_inf = (normalisation(y_train_k_inf))\n",
    "y_train_pppf = (normalisation(y_train_pppf))\n",
    "\n",
    "\n",
    "y_train_k_inf = y_train_k_inf.ravel() \n",
    "y_train_pppf = y_train_pppf.ravel()\n",
    "\n",
    "y_test_k_inf = (df_test.iloc[:, :1]).to_numpy()\n",
    "y_test_pppf = (df_test.iloc[:, 1:2]).to_numpy()\n",
    "x_test = (df_test.iloc[:, 2:]).to_numpy()\n",
    "\n",
    "\n",
    "x_test = (normalisation(x_test))\n",
    "y_test_k_inf = (normalisation(y_test_k_inf))\n",
    "y_test_pppf = (normalisation(y_test_pppf))\n",
    "\n",
    "\n",
    "y_test_k_inf = y_test_k_inf.ravel() \n",
    "y_test_pppf = y_test_pppf.ravel()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression\n",
    "The linear regression is used for training the model using the regression function which is a seperate script. The current linear regression works on the principle of reducing the cost function (squared error cost function given below), this is done automatically by an algorithm known as gradient descent.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_k_inf,b_k_inf,cost_k_inf = regression_func(x_train, y_train_k_inf, 10000,0.1)\n",
    "w_pppf,b_k_pppf,cost_pppf = regression_func(x_train, y_train_k_inf, 10000,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k_inf - Mean Squared Error (Cost): 0.01837588317990346, R_2 Score: 0.9816241168200965\n",
      "pppf - Mean Squared Error (Cost): 3.752469363008063, R_2 Score: -2.752469363008063\n"
     ]
    }
   ],
   "source": [
    "def predict(x, w, b):\n",
    "    return np.dot(x, w) + b\n",
    "y_pred_k_inf = predict(x_test, w_k_inf, b_k_inf)\n",
    "y_pred_pppf = predict(x_test, w_pppf, b_k_pppf)\n",
    "\n",
    "mse_k_inf = mean_squared_error(y_test_k_inf, y_pred_k_inf)\n",
    "r2_k_inf = r2_score(y_test_k_inf, y_pred_k_inf)\n",
    "\n",
    "mse_pppf = mean_squared_error(y_test_pppf, y_pred_pppf)\n",
    "r2_pppf = r2_score(y_test_pppf, y_pred_pppf)\n",
    "\n",
    "\n",
    "print(f\"k_inf - Mean Squared Error (Cost): {mse_k_inf}, R_2 Score: {r2_k_inf}\")\n",
    "print(f\"pppf - Mean Squared Error (Cost): {mse_pppf}, R_2 Score: {r2_pppf}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Regression\n",
    "The linear regression used for training the model for the target variable PPPF has failed misearbly, this can only mean that data is incompatible with linear regression thus we have to switch the data for polynomial regression. The mean squared error with a polynomial regression of degree 3 reduces quite well, also The R_2 score remains at 0.60 meaning the model is able to predict the PPPF in a much better way than before. Natuarally the model can be improved drastically by using a neural network model to have even better R_2 scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.3929\n",
      "R^2 Score: 0.6071\n"
     ]
    }
   ],
   "source": [
    "\n",
    "poly = PolynomialFeatures(degree=3, include_bias=False)\n",
    "x_polynomial = poly.fit_transform(x_train)\n",
    "\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(x_train, y_train_pppf)\n",
    "\n",
    "ridge = Ridge(alpha=1.0) \n",
    "ridge.fit(x_polynomial, y_train_pppf)\n",
    "\n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "mse = mean_squared_error(y_test_pppf, y_pred)\n",
    "r2 = r2_score(y_test_pppf, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "print(f\"R^2 Score: {r2:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
